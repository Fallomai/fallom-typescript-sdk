/**
 * Type definitions for Fallom Evals.
 */

/** Built-in metric names */
export type MetricName =
  | "answer_relevancy"
  | "hallucination"
  | "toxicity"
  | "faithfulness"
  | "completeness"
  | "coherence"
  | "bias";

/** List of all available built-in metrics */
export const AVAILABLE_METRICS: MetricName[] = [
  "answer_relevancy",
  "hallucination",
  "toxicity",
  "faithfulness",
  "completeness",
  "coherence",
  "bias",
];

/**
 * Define a custom evaluation metric using G-Eval.
 */
export interface CustomMetric {
  /** Unique identifier for the metric (e.g., "brand_alignment") */
  name: string;
  /** Description of what the metric evaluates */
  criteria: string;
  /** List of evaluation steps for the LLM judge to follow */
  steps: string[];
}

/** Metric can be a built-in name or a custom metric */
export type MetricInput = MetricName | CustomMetric;

/** Dataset can be a list of items OR a string (dataset key to fetch from Fallom) */
export type DatasetInput = DatasetItem[] | string;

/** A single item in an evaluation dataset */
export interface DatasetItem {
  input: string;
  output: string;
  systemMessage?: string;
  metadata?: Record<string, unknown>;
}

/** Evaluation result for a single item */
export interface EvalResult {
  input: string;
  output: string;
  systemMessage?: string;
  /** Expected/golden output for comparison (if provided) */
  expectedOutput?: string;
  /** Retrieved documents/context for RAG evaluation */
  context?: string[];
  /** Additional metadata */
  metadata?: Record<string, unknown>;
  model: string;
  isProduction: boolean;
  answerRelevancy?: number;
  hallucination?: number;
  toxicity?: number;
  faithfulness?: number;
  completeness?: number;
  coherence?: number;
  bias?: number;
  reasoning: Record<string, string>;
  latencyMs?: number;
  tokensIn?: number;
  tokensOut?: number;
  cost?: number;
}

/**
 * A golden record from a dataset - contains input and optionally expected output.
 * This represents a "golden" test case that you can use to generate actual outputs
 * from your LLM pipeline and then evaluate.
 */
export interface Golden {
  input: string;
  expectedOutput?: string;
  systemMessage?: string;
  /** Retrieved documents for RAG evaluation */
  context?: string[];
  metadata?: Record<string, unknown>;
}

/**
 * A test case for evaluation - contains input and actual output from your LLM.
 *
 */
export interface LLMTestCase {
  /** The user input/query */
  input: string;
  /** The output generated by your LLM pipeline */
  actualOutput: string;
  /** (Optional) The expected/golden output for comparison */
  expectedOutput?: string;
  /** (Optional) System prompt used */
  systemMessage?: string;
  /** (Optional) Retrieved documents for RAG faithfulness evaluation */
  context?: string[];
  /** (Optional) Additional metadata */
  metadata?: Record<string, unknown>;
}

/** Response format from model calls */
export interface ModelResponse {
  content: string;
  tokensIn?: number;
  tokensOut?: number;
  cost?: number;
}

/** Message format for model calls */
export interface Message {
  role: "system" | "user" | "assistant";
  content: string;
}

/** Callable type for custom models */
export type ModelCallable = (messages: Message[]) => Promise<ModelResponse>;

/**
 * A model configuration for use in compareModels().
 * Can represent either an OpenRouter model or a custom model (fine-tuned, self-hosted)
 */
export interface Model {
  name: string;
  callFn?: ModelCallable;
}

/** Options for init() */
export interface InitOptions {
  apiKey?: string;
  baseUrl?: string;
}

/** Options for evaluate() */
export interface EvaluateOptions {
  /** Dataset to evaluate (list of DatasetItem or Fallom dataset key) */
  dataset?: DatasetInput;
  /** List of metrics to run (built-in or custom). Default: all built-in metrics */
  metrics?: MetricInput[];
  judgeModel?: string;
  name?: string;
  description?: string;
  verbose?: boolean;
  /** Alternative to dataset - provide test cases from EvaluationDataset */
  testCases?: LLMTestCase[];
  _skipUpload?: boolean;
}

/** Options for compareModels() */
export interface CompareModelsOptions extends EvaluateOptions {
  /**
   * List of models to test. Each can be:
   * - A string (model slug for OpenRouter, e.g., "anthropic/claude-3-5-sonnet")
   * - A Model object (for custom/fine-tuned models)
   */
  models: Array<string | Model>;
  includeProduction?: boolean;
  modelKwargs?: Record<string, unknown>;
}

/** Type guard to check if a metric is a CustomMetric */
export function isCustomMetric(metric: MetricInput): metric is CustomMetric {
  return typeof metric === "object" && "name" in metric && "criteria" in metric;
}

/** Get the name of a metric (works for both built-in and custom) */
export function getMetricName(metric: MetricInput): string {
  return isCustomMetric(metric) ? metric.name : metric;
}
